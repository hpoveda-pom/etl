# Suite ETL - Procesamiento de Datos

Suite completa de herramientas ETL para procesar datos desde m√∫ltiples fuentes (SQL Server, Excel) hacia diferentes destinos (CSV, ClickHouse, Snowflake).

---

## Informaci√≥n del Proyecto

- **Autor**: Herbert Poveda
- **Empresa**: POM Cobranzas
- **Departamento**: Business Intelligence (BI)
- **Fecha**: 19 de enero de 2026
- **Versi√≥n**: 1.0.0

---

## Tabla de Contenidos

- [Descripci√≥n General](#descripci√≥n-general)
- [Requisitos](#requisitos)
- [Instalaci√≥n](#instalaci√≥n)
- [Configuraci√≥n con archivo .env](#configuraci√≥n-con-archivo-env)
- [Estructura de Carpetas](#estructura-de-carpetas)
- [Scripts ETL](#scripts-etl)
  - [sqlserver_to_csv.py](#1-sqlserver_to_csvpy)
  - [excel_to_csv.py](#2-excel_to_csvpy)
  - [compress_csv_to_gz.py](#3-compress_csv_to_gzpy)
  - [csv_to_snowflake.py](#4-csv_to_snowflakepy)
  - [csv_to_clickhouse.py](#5-csv_to_clickhousepy)
  - [ingest_all_excels_to_stage.py](#6-ingest_all_excels_to_stagepy)
  - [snowflake_csv_to_tables.py](#7-snowflake_csv_to_tablespy)
  - [sqlserver_to_snowflake_streaming.py](#8-sqlserver_to_snowflake_streamingpy)
  - [sqlserver_to_clickhouse_streaming.py](#9-sqlserver_to_clickhouse_streamingpy)
  - [clickhouse_csv_to_tables.py](#10-clickhouse_csv_to_tablespy)
  - [snowflake_drop_tables.py](#11-snowflake_drop_tablespy)
  - [clickhouse_drop_tables.py](#12-clickhouse_drop_tablespy)
- [Flujos de Trabajo Comunes](#flujos-de-trabajo-comunes)
- [Troubleshooting](#troubleshooting)
- [Changelog](#changelog)

---

## Descripci√≥n General

Esta suite ETL proporciona herramientas para:

- **Extracci√≥n**: Exportar datos desde SQL Server y Excel
- **Transformaci√≥n**: Convertir formatos, comprimir archivos, filtrar datos
- **Carga**: Importar datos a ClickHouse y Snowflake

### Caracter√≠sticas Principales

- Exportaci√≥n masiva de tablas SQL Server a CSV
- Conversi√≥n de archivos Excel a CSV
- Compresi√≥n de CSV a formato GZ
- Carga autom√°tica a Snowflake y ClickHouse
- **Streaming directo SQL Server -> ClickHouse/Snowflake (sin CSV intermedio)**
- **Carga incremental inteligente** (por ID, timestamp o hash de fila)
- **Deduplicaci√≥n autom√°tica** con ReplacingMergeTree
- **Manejo de updates y deletes** con lookback window
- Filtrado flexible por tablas, carpetas y archivos
- Manejo robusto de errores con reintentos
- Logging detallado de operaciones
- Sanitizaci√≥n autom√°tica de nombres

---

## Requisitos

### Requisitos Generales

- **Python 3.7+**
- **Sistema Operativo**: Windows (principalmente), Linux/Mac compatible

### Dependencias Python

```bash
pip install pandas pyodbc openpyxl clickhouse-connect snowflake-connector-python python-dotenv
```

O instalar todas de una vez:

```bash
pip install -r requirements.txt
```

**Nota**: `python-dotenv` es opcional pero recomendado para usar archivos `.env`. Si no est√° instalado, los scripts usar√°n variables de entorno del sistema.

### Requisitos Adicionales por Script

| Script | Requisitos Adicionales |
|--------|----------------------|
| `sqlserver_to_csv.py` | Driver ODBC para SQL Server |
| `excel_to_csv.py` | openpyxl (incluido en dependencias) |
| `csv_to_snowflake.py` | Cuenta de Snowflake |
| `csv_to_clickhouse.py` | Cuenta de ClickHouse Cloud |
| `ingest_all_excels_to_stage.py` | Cuenta de Snowflake |
| `sqlserver_to_snowflake_streaming.py` | Driver ODBC, Cuenta de Snowflake, pandas |
| `sqlserver_to_clickhouse_streaming.py` | Driver ODBC, Cuenta de ClickHouse, clickhouse-connect |
| `snowflake_csv_to_tables.py` | Cuenta de Snowflake |
| `clickhouse_csv_to_tables.py` | Cuenta de ClickHouse Cloud |
| `snowflake_drop_tables.py` | Cuenta de Snowflake |
| `clickhouse_drop_tables.py` | Cuenta de ClickHouse Cloud |

### Drivers ODBC para SQL Server

Necesitas uno de los siguientes drivers instalados:
- ODBC Driver 17 for SQL Server (recomendado)
- ODBC Driver 13 for SQL Server
- SQL Server Native Client 11.0
- SQL Server

---

## Instalaci√≥n

1. **Clonar o descargar el repositorio**

2. **Instalar dependencias**:
```bash
pip install pandas pyodbc openpyxl clickhouse-connect snowflake-connector-python python-dotenv
```

3. **Configurar archivo `.env`** (ver secci√≥n [Configuraci√≥n con archivo .env](#configuraci√≥n-con-archivo-env))

4. **Crear estructura de carpetas** (se crean autom√°ticamente, pero puedes crearlas manualmente):
```
UPLOADS/
‚îî‚îÄ‚îÄ POM_DROP/
    ‚îú‚îÄ‚îÄ inbox/          # Archivos Excel a procesar
    ‚îú‚îÄ‚îÄ processed/      # Archivos procesados exitosamente
    ‚îú‚îÄ‚îÄ error/          # Archivos con errores
    ‚îú‚îÄ‚îÄ csv_staging/    # CSV generados (intermedio)
    ‚îú‚îÄ‚îÄ csv_processed/  # CSV procesados exitosamente
    ‚îî‚îÄ‚îÄ csv_error/      # CSV con errores
```

---

## Configuraci√≥n con archivo .env

**Recomendado**: Usar un archivo `.env` para gestionar todas las credenciales y configuraciones de forma segura.

### Paso 1: Crear archivo .env

Copia el archivo de ejemplo y personal√≠zalo:

```bash
# Windows (PowerShell)
copy .env.example .env

# Linux/Mac
cp .env.example .env
```

### Paso 2: Editar .env con tus credenciales

Abre el archivo `.env` y configura tus credenciales:

```env
# ============================================
# Configuraci√≥n SQL Server
# ============================================
SQL_SERVER=SRV-DESA\SQLEXPRESS
SQL_DATABASE=MiBaseDeDatos
SQL_USER=tu_usuario         # Requerido por defecto (autenticaci√≥n SQL Server)
SQL_PASSWORD=tu_password    # Requerido por defecto (autenticaci√≥n SQL Server)
SQL_USE_WINDOWS_AUTH=false  # Opcional: true para usar autenticaci√≥n Windows
SQL_DRIVER=ODBC Driver 17 for SQL Server

# ============================================
# Configuraci√≥n ClickHouse Cloud
# ============================================
CH_HOST=f4rf85ygzj.eastus2.azure.clickhouse.cloud
CH_PORT=8443
CH_USER=default
CH_PASSWORD=tu_password_aqui  # [WARN] OBLIGATORIO
CH_DATABASE=default

# ============================================
# Configuraci√≥n Snowflake
# ============================================
SF_ACCOUNT=fkwugeu-qic97823
SF_USER=HPOVEDAPOMCR
SF_PASSWORD=tu_password_aqui  # [WARN] OBLIGATORIO
SF_ROLE=ACCOUNTADMIN
SF_WAREHOUSE=COMPUTE_WH
SF_DATABASE=POM_TEST01
SF_SCHEMA=RAW

# ============================================
# Configuraci√≥n de Streaming
# ============================================
STREAMING_CHUNK_SIZE=10000
TARGET_TABLE_PREFIX=          # Vac√≠o por defecto (sin prefijo)
TABLES_FILTER=                # Opcional: Tabla1,Tabla2
LOOKBACK_DAYS=7               # D√≠as hacia atr√°s para detectar updates
USE_REPLACING_MERGE_TREE=true # Usar ReplacingMergeTree en ClickHouse
CH_TIMEZONE=UTC               # Zona horaria para DateTime64

# ============================================
# Configuraci√≥n de Carpetas
# ============================================
INBOX_DIR=UPLOADS\POM_DROP\inbox
PROCESSED_DIR=UPLOADS\POM_DROP\processed
ERROR_DIR=UPLOADS\POM_DROP\error
CSV_STAGING_DIR=UPLOADS\POM_DROP\csv_staging
CSV_PROCESSED_DIR=UPLOADS\POM_DROP\csv_processed
CSV_ERROR_DIR=UPLOADS\POM_DROP\csv_error
```

### Paso 3: Verificar que funciona

Al ejecutar cualquier script, ver√°s un mensaje si el `.env` se carg√≥ correctamente:

```
[OK] Archivo .env cargado desde: C:\xampp\htdocs\etl\.env
```

### Variables de Entorno Alternativas

Si prefieres no usar `.env`, puedes configurar variables de entorno del sistema:

**Windows (PowerShell)**:
```powershell
$env:CH_PASSWORD="tu_password"
$env:SQL_SERVER="SRV-DESA\SQLEXPRESS"
```

**Windows (CMD)**:
```cmd
set CH_PASSWORD=tu_password
set SQL_SERVER=SRV-DESA\SQLEXPRESS
```

**Linux/Mac**:
```bash
export CH_PASSWORD="tu_password"
export SQL_SERVER="SRV-DESA\\SQLEXPRESS"
```

### Seguridad del archivo .env

[WARN] **IMPORTANTE**: 
- **NUNCA** subas el archivo `.env` a Git
- Agrega `.env` a tu `.gitignore`
- El archivo `.env.example` puede estar en Git (sin credenciales)
- Mant√©n permisos restrictivos en el archivo `.env`

---

## Estructura de Carpetas

El sistema utiliza la siguiente estructura de carpetas (configurable mediante variables de entorno o `.env`):

```
UPLOADS/POM_DROP/
‚îú‚îÄ‚îÄ inbox/              # Archivos Excel (.xlsx) a procesar
‚îú‚îÄ‚îÄ processed/          # Archivos Excel procesados exitosamente
‚îú‚îÄ‚îÄ error/              # Archivos Excel con errores
‚îú‚îÄ‚îÄ csv_staging/        # CSV generados (punto intermedio)
‚îÇ   ‚îú‚îÄ‚îÄ SQLSERVER_*     # Carpetas con CSV de SQL Server
‚îÇ   ‚îî‚îÄ‚îÄ [nombre_excel] # Carpetas con CSV de Excel
‚îú‚îÄ‚îÄ csv_processed/      # Carpetas de CSV procesadas exitosamente
‚îî‚îÄ‚îÄ csv_error/          # Carpetas de CSV con errores
```

### Flujo de Archivos

1. **Excel -> CSV**: `inbox/` -> `csv_staging/` -> `processed/` o `error/`
2. **SQL Server -> CSV**: `csv_staging/SQLSERVER_*/` -> (procesamiento) -> `csv_processed/` o `csv_error/`
3. **CSV -> Cloud**: `csv_staging/` -> (carga) -> `csv_processed/` o `csv_error/`

---

## Scripts ETL

### 1. sqlserver_to_csv.py

**Versi√≥n**: 1.0.0  
**Descripci√≥n**: Exporta tablas de SQL Server a archivos CSV.

**Requisitos**:
- Driver ODBC para SQL Server
- Acceso a SQL Server (Windows Auth o SQL Auth)

**Variables de Entorno** (o `.env`):
- `SQL_SERVER`, `SQL_DATABASE` (requeridos)
- `SQL_USER`, `SQL_PASSWORD` (requeridos por defecto, ver nota abajo)
- `SQL_USE_WINDOWS_AUTH` (opcional, default: `false` - si es `true`, no requiere SQL_USER/SQL_PASSWORD)
- `SQL_DRIVER` (opcional, default: "ODBC Driver 17 for SQL Server")
- `CSV_STAGING_DIR`
- `TABLES_FILTER` (opcional)

**Uso**:

```bash
# Exportar todas las tablas de una base de datos
python sqlserver_to_csv.py MiBaseDeDatos

# Exportar tablas espec√≠ficas
python sqlserver_to_csv.py MiBaseDeDatos Tabla1,Tabla2,Tabla3
```

**Caracter√≠sticas**:
- **Autenticaci√≥n SQL Server por defecto** (requiere SQL_USER y SQL_PASSWORD)
- Para usar autenticaci√≥n Windows, define `SQL_USE_WINDOWS_AUTH=true` en `.env`
- Detecci√≥n autom√°tica de drivers ODBC
- Exclusi√≥n de tablas por prefijos
- Filtrado de tablas espec√≠ficas
- Reintentos autom√°ticos en caso de p√©rdida de conexi√≥n
- Sanitizaci√≥n autom√°tica de nombres de archivos

**Salida**: Archivos CSV en `CSV_STAGING_DIR/SQLSERVER_[nombre_base_datos]/`

---

### 2. excel_to_csv.py

**Versi√≥n**: 1.0.0  
**Descripci√≥n**: Convierte archivos Excel (.xlsx) a CSV. Cada hoja del Excel se convierte en un archivo CSV separado.

**Requisitos**:
- `openpyxl` (incluido en dependencias)

**Variables de Entorno** (o `.env`):
- `INBOX_DIR`, `PROCESSED_DIR`, `ERROR_DIR`
- `CSV_STAGING_DIR`
- `SHEETS_ALLOWLIST` (opcional)

**Uso**:

```bash
# Procesar todos los Excel en inbox
python excel_to_csv.py
```

**Caracter√≠sticas**:
- Procesa todos los archivos `.xlsx` en `INBOX_DIR`
- Cada hoja del Excel se convierte en un CSV separado
- Crea una carpeta con el nombre del Excel (sanitizado)
- Mueve archivos a `processed/` o `error/` seg√∫n resultado
- Manejo de archivos bloqueados con reintentos

**Salida**: 
- Archivos CSV en `CSV_STAGING_DIR/[nombre_excel]/[hoja].csv`
- Excel original movido a `processed/` o `error/`

---

### 3. compress_csv_to_gz.py

**Versi√≥n**: 1.0.0  
**Descripci√≥n**: Comprime archivos CSV a formato `.csv.gz` para reducir el tama√±o.

**Requisitos**: Ninguno adicional

**Variables de Entorno** (o `.env`):
- `CSV_STAGING_DIR`
- `FOLDERS_FILTER` (opcional)
- `CSV_FILTER` (opcional)
- `DELETE_ORIGINALS` (opcional, `true`/`false`)

**Uso**:

```bash
# Comprimir todos los CSV
python compress_csv_to_gz.py

# Comprimir CSV de una carpeta espec√≠fica
python compress_csv_to_gz.py SQLSERVER_POM_Aplicaciones

# Comprimir CSV espec√≠ficos
python compress_csv_to_gz.py SQLSERVER_POM_Aplicaciones ResutadoNotificar,Bitacora
```

**Caracter√≠sticas**:
- Comprime CSV a formato GZ (compatible con Snowflake y ClickHouse)
- Muestra ratio de compresi√≥n
- Opci√≥n para eliminar CSV originales despu√©s de comprimir exitosamente
- Filtrado por carpetas y archivos
- Omite archivos que ya tienen su versi√≥n `.csv.gz`

**Salida**: Archivos `.csv.gz` en las mismas carpetas

---

### 4. csv_to_snowflake.py

**Versi√≥n**: 1.0.0  
**Descripci√≥n**: Carga archivos CSV (o CSV.gz) desde `csv_staging` a Snowflake.

**Requisitos**:
- Cuenta de Snowflake
- `snowflake-connector-python`

**Variables de Entorno** (o `.env`):
- `SF_ACCOUNT`, `SF_USER`, `SF_PASSWORD`, `SF_ROLE`, `SF_WAREHOUSE`
- `SF_DATABASE`, `SF_SCHEMA`
- `CSV_STAGING_DIR`, `CSV_PROCESSED_DIR`, `CSV_ERROR_DIR`
- `FOLDERS_FILTER`, `CSV_FILTER` (opcional)

**Uso**:

```bash
# Cargar todos los CSV a Snowflake
python csv_to_snowflake.py

# Especificar base de datos y schema
python csv_to_snowflake.py POM_TEST01 RAW

# Filtrar por carpeta
python csv_to_snowflake.py POM_TEST01 RAW SQLSERVER_POM_Aplicaciones
```

**Caracter√≠sticas**:
- Crea autom√°ticamente stage `RAW_STAGE` si no existe
- Crea tablas `INGEST_LOG` e `INGEST_GENERIC_RAW` en schema RAW
- Comprime CSV a `.gz` autom√°ticamente antes de subir
- Registra operaciones en `INGEST_LOG`
- Mueve carpetas procesadas a `csv_processed/` o `csv_error/`

**Salida**: 
- Datos cargados en Snowflake
- Carpetas movidas a `csv_processed/` o `csv_error/`

---

### 5. csv_to_clickhouse.py

**Versi√≥n**: 1.0.0  
**Descripci√≥n**: Carga archivos CSV (o CSV.gz) desde `csv_staging` a ClickHouse Cloud.

**Requisitos**:
- Cuenta de ClickHouse Cloud
- `clickhouse-connect`

**Variables de Entorno** (o `.env`):
- `CH_HOST`, `CH_PORT`, `CH_USER`, `CH_PASSWORD`, `CH_DATABASE`
- `CH_TABLE` (opcional)
- `CSV_STAGING_DIR`, `CSV_PROCESSED_DIR`, `CSV_ERROR_DIR`
- `FOLDERS_FILTER`, `CSV_FILTER` (opcional)

**Uso**:

```bash
# Cargar todos los CSV a ClickHouse
python csv_to_clickhouse.py

# Especificar base de datos
python csv_to_clickhouse.py default

# Filtrar por carpeta
python csv_to_clickhouse.py default SQLSERVER_POM_Aplicaciones
```

**Caracter√≠sticas**:
- Crea tablas autom√°ticamente con estructura del CSV
- Detecta delimitador autom√°ticamente (coma, punto y coma, tab, pipe)
- Maneja columnas duplicadas renombr√°ndolas
- Soporta CSV comprimidos (.csv.gz)
- Sanitiza nombres de columnas y tablas
- Mueve carpetas procesadas a `csv_processed/` o `csv_error/`

**Salida**: 
- Tablas creadas en ClickHouse con los datos cargados
- Carpetas movidas a `csv_processed/` o `csv_error/`

---

### 6. ingest_all_excels_to_stage.py

**Versi√≥n**: 1.0.0  
**Descripci√≥n**: Procesa archivos Excel directamente desde `inbox` y los carga a Snowflake en un solo paso.

**Requisitos**:
- Cuenta de Snowflake
- `snowflake-connector-python`
- `openpyxl`

**Variables de Entorno** (o `.env`):
- `SF_ACCOUNT`, `SF_USER`, `SF_PASSWORD`, `SF_ROLE`, `SF_WAREHOUSE`
- `SF_DATABASE`, `SF_SCHEMA`
- `INBOX_DIR`, `PROCESSED_DIR`, `ERROR_DIR`
- `SHEETS_ALLOWLIST` (opcional)

**Uso**:

```bash
# Procesar todos los Excel en inbox
python ingest_all_excels_to_stage.py
```

**Caracter√≠sticas**:
- Lee Excel desde `inbox/`
- Convierte cada hoja a CSV comprimido (.csv.gz)
- Sube directamente al stage de Snowflake
- Carga datos a `INGEST_GENERIC_RAW`
- Registra operaciones en `INGEST_LOG`
- Mueve Excel a `processed/` o `error/` seg√∫n resultado

**Salida**: 
- Datos cargados en Snowflake
- Excel movido a `processed/` o `error/`

---

### 7. snowflake_csv_to_tables.py

**Versi√≥n**: 1.0.0  
**Descripci√≥n**: Crea tablas individuales en Snowflake desde archivos CSV que ya est√°n en el stage.

**Requisitos**:
- Cuenta de Snowflake
- Archivos CSV ya cargados en el stage `RAW_STAGE`

**Variables de Entorno** (o `.env`):
- `SF_ACCOUNT`, `SF_USER`, `SF_PASSWORD`, `SF_ROLE`, `SF_WAREHOUSE`
- `SF_DATABASE`, `SF_SCHEMA`

**Uso**:

```bash
# Crear tablas desde todos los CSV en el stage
python snowflake_csv_to_tables.py

# Especificar base de datos y schema
python snowflake_csv_to_tables.py POM_TEST01 RAW
```

**Caracter√≠sticas**:
- Lee archivos CSV desde el stage de Snowflake
- Crea una tabla por cada archivo CSV
- El nombre de la tabla es el nombre del archivo (sin extensi√≥n)
- Omite tablas que ya existen
- Detecta headers autom√°ticamente
- Carga datos desde el stage a la tabla

**Salida**: Tablas creadas en Snowflake con los datos de los CSV

---

### 8. sqlserver_to_snowflake_streaming.py

**Versi√≥n**: 1.0.0  
**Descripci√≥n**: Exporta tablas de SQL Server directamente a Snowflake usando streaming (sin pasar por CSV intermedio).

**Requisitos**:
- Driver ODBC para SQL Server
- Cuenta de Snowflake
- `snowflake-connector-python`
- `pandas`

**Variables de Entorno** (o `.env`):
- `SQL_SERVER`, `SQL_DATABASE` (requeridos)
- `SQL_USER`, `SQL_PASSWORD` (requeridos por defecto, ver nota abajo)
- `SQL_USE_WINDOWS_AUTH` (opcional, default: `false` - si es `true`, no requiere SQL_USER/SQL_PASSWORD)
- `SQL_DRIVER` (opcional, default: "ODBC Driver 17 for SQL Server")
- `SF_ACCOUNT`, `SF_USER`, `SF_PASSWORD`, `SF_ROLE`, `SF_WAREHOUSE`
- `SF_DATABASE`, `SF_SCHEMA`
- `STREAMING_CHUNK_SIZE` (opcional, default: 10000)
- `TARGET_TABLE_PREFIX` (opcional, default: "SQLSERVER_")
- `TABLES_FILTER` (opcional)

**Uso**:

```bash
# Exportar todas las tablas usando variables de entorno
python sqlserver_to_snowflake_streaming.py

# Especificar base de datos SQL Server y Snowflake
python sqlserver_to_snowflake_streaming.py POM_DBS POM_TEST01 RAW

# Exportar tablas espec√≠ficas
python sqlserver_to_snowflake_streaming.py POM_DBS POM_TEST01 RAW "Tabla1,Tabla2"
```

**Caracter√≠sticas**:
- Streaming directo desde SQL Server a Snowflake (sin archivos intermedios)
- Crea tablas autom√°ticamente bas√°ndose en la estructura de SQL Server
- Mapea tipos de datos de SQL Server a Snowflake
- Procesa datos en chunks para manejar grandes vol√∫menes
- Agrega columna `ingested_at` con timestamp autom√°tico
- Excluye tablas por prefijos (configurable)

**Salida**: Tablas creadas en Snowflake con datos cargados directamente desde SQL Server

---

### 9. sqlserver_to_clickhouse_streaming.py 

**Versi√≥n**: 2.0.0  
**Descripci√≥n**: Exporta tablas de SQL Server directamente a ClickHouse usando streaming con carga incremental inteligente.

**Requisitos**:
- Driver ODBC para SQL Server
- Cuenta de ClickHouse Cloud
- `clickhouse-connect`
- `python-dotenv` (recomendado para `.env`)

**Variables de Entorno** (o `.env`):
- `SQL_SERVER`, `SQL_DATABASE` (requeridos)
- `SQL_USER`, `SQL_PASSWORD` (requeridos por defecto, ver nota abajo)
- `SQL_USE_WINDOWS_AUTH` (opcional, default: `false` - si es `true`, no requiere SQL_USER/SQL_PASSWORD)
- `SQL_DRIVER` (opcional, default: "ODBC Driver 17 for SQL Server")
- `CH_HOST`, `CH_PORT`, `CH_USER`, `CH_PASSWORD`, `CH_DATABASE` [WARN] **OBLIGATORIO**
- `STREAMING_CHUNK_SIZE` (opcional, default: 10000)
- `TARGET_TABLE_PREFIX` (opcional, default: "" - sin prefijo)
- `TABLES_FILTER` (opcional)
- `LOOKBACK_DAYS` (opcional, default: 7 - d√≠as para detectar updates)
- `USE_REPLACING_MERGE_TREE` (opcional, default: true)
- `CH_TIMEZONE` (opcional, default: UTC)

**Uso**:

```bash
# Exportar tabla espec√≠fica con l√≠mite de registros
python sqlserver_to_clickhouse_streaming.py POM_Aplicaciones POM_Aplicaciones_test "PC_Gestiones" 5

# Exportar todas las tablas (modo incremental autom√°tico)
python sqlserver_to_clickhouse_streaming.py POM_Aplicaciones POM_Aplicaciones_test

# Exportar tabla espec√≠fica (sin l√≠mite)
python sqlserver_to_clickhouse_streaming.py POM_Aplicaciones POM_Aplicaciones_test "PC_Gestiones"
```

**Caracter√≠sticas Avanzadas**:

####  Carga Incremental Inteligente
- **Detecci√≥n autom√°tica** de columna ID o timestamp
- **Modo ID**: Procesa solo registros nuevos basado en ID incremental
- **Modo Timestamp**: Procesa solo registros nuevos basado en fecha/hora
- **Modo Hash**: Para tablas sin ID ni fecha, usa hash MD5 de la fila completa
- **Lookback Window**: Detecta updates/deletes en los √∫ltimos N d√≠as

#### üöÄ Optimizaciones de Rendimiento
- **Streaming por chunks**: Procesa datos en lotes configurables
- **Verificaci√≥n de hashes por chunk**: Escalable, no carga todos los hashes
- **Sin pandas**: Trabajo directo con listas para mejor rendimiento
- **Medici√≥n de tiempo**: Muestra duraci√≥n y velocidad de cada chunk

#### üîí Deduplicaci√≥n Autom√°tica
- **ReplacingMergeTree**: Engine de ClickHouse para deduplicaci√≥n autom√°tica
- **ORDER BY correcto**: Por `row_hash` (modo hash), `id` (modo ID) o `timestamp` (modo fecha)
- **Deduplicaci√≥n antes de insertar**: Filtra duplicados en memoria antes de cargar

####  Monitoreo y Logging
- Muestra tiempo de procesamiento por chunk
- Velocidad de procesamiento (filas/segundo)
- Resumen final con estad√≠sticas
- Detecci√≥n de updates vs nuevos registros

**Ejemplo de Salida**:
```
[OK] Archivo .env cargado desde: C:\xampp\htdocs\etl\.env
[OK] Conectado a SQL Server: SRV-DESA\SQLEXPRESS/POM_Aplicaciones
[OK] Conectado a ClickHouse: f4rf85ygzj.eastus2.azure.clickhouse.cloud:8443/POM_Aplicaciones_test
Modo: INCREMENTAL (columna ID: Id)
-> Exportando: dbo.PC_Gestiones -> PC_Gestiones
Usando ReplacingMergeTree con versi√≥n: ingested_at
 ORDER BY: Id (para deduplicaci√≥n por ID)
[OK] Tabla creada: PC_Gestiones (9 columnas)
Lookback window (7 d√≠as): 10 IDs en rango (para detectar updates)
Modo incremental (ID): √∫ltimo valor procesado = 10
 Iniciando streaming (chunk size: 10000)...
[OK] Chunk 1: 10000 filas insertadas (total: 10000) [1.23s] 8,130 filas/s
[OK] Chunk 2: 10000 filas insertadas (total: 20000) [1.18s] 8,475 filas/s
  Tiempo total: 2m 15.3s | Tiempo promedio por chunk: 1.22s | Velocidad: 8,197 filas/s
[OK] Exportaci√≥n completada: 1 tablas exportadas
```

**Salida**: Tablas creadas en ClickHouse con datos cargados directamente desde SQL Server, con deduplicaci√≥n autom√°tica y carga incremental

---

### 10. clickhouse_csv_to_tables.py

**Versi√≥n**: 1.0.0  
**Descripci√≥n**: Crea tablas individuales en ClickHouse desde archivos CSV locales.

**Requisitos**:
- Cuenta de ClickHouse Cloud
- `clickhouse-connect`
- Archivos CSV en `CSV_STAGING_DIR`

**Variables de Entorno** (o `.env`):
- `CH_HOST`, `CH_PORT`, `CH_USER`, `CH_PASSWORD`, `CH_DATABASE`
- `CSV_STAGING_DIR`

**Uso**:

```bash
# Crear tablas desde todos los CSV en staging
python clickhouse_csv_to_tables.py

# Especificar base de datos
python clickhouse_csv_to_tables.py default

# Filtrar por carpeta
python clickhouse_csv_to_tables.py default CIERRE_PROPIAS___7084110
```

**Caracter√≠sticas**:
- Lee archivos CSV desde el directorio local
- Crea una tabla por cada archivo CSV
- El nombre de la tabla es el nombre del archivo (sin extensi√≥n)
- Detecta delimitador autom√°ticamente
- Maneja columnas duplicadas renombr√°ndolas
- Soporta CSV comprimidos (.csv.gz) y sin comprimir
- Omite tablas que ya existen (o las reemplaza si tienen estructura diferente)

**Salida**: Tablas creadas en ClickHouse con los datos de los CSV

---

### 11. snowflake_drop_tables.py

**Versi√≥n**: 1.0.0  
**Descripci√≥n**: Elimina tablas en Snowflake de forma segura con confirmaci√≥n.

**Requisitos**:
- Cuenta de Snowflake
- `snowflake-connector-python`

**Variables de Entorno** (o `.env`):
- `SF_ACCOUNT`, `SF_USER`, `SF_PASSWORD`, `SF_ROLE`, `SF_WAREHOUSE`
- `SF_DATABASE`, `SF_SCHEMA`
- `REQUIRE_CONFIRMATION` (opcional, default: "true")

**Uso**:

```bash
# Eliminar tablas espec√≠ficas (requiere confirmaci√≥n)
python snowflake_drop_tables.py POM_TEST01 RAW Tabla1,Tabla2,Tabla3

# Eliminar tablas por patr√≥n
python snowflake_drop_tables.py POM_TEST01 RAW "PC_%"
```

**Caracter√≠sticas**:
- Confirmaci√≥n de seguridad por defecto
- Soporta filtros por patr√≥n (LIKE)
- Soporta lista espec√≠fica de tablas
- Opci√≥n para eliminar todas las tablas del schema
- Opci√≥n para omitir confirmaci√≥n (√∫til para scripts automatizados)
- Muestra lista de tablas a eliminar antes de confirmar

**Salida**: Tablas eliminadas en Snowflake

---

### 12. clickhouse_drop_tables.py

**Versi√≥n**: 1.0.0  
**Descripci√≥n**: Elimina tablas en ClickHouse de forma segura con confirmaci√≥n.

**Requisitos**:
- Cuenta de ClickHouse Cloud
- `clickhouse-connect`

**Variables de Entorno** (o `.env`):
- `CH_HOST`, `CH_PORT`, `CH_USER`, `CH_PASSWORD`, `CH_DATABASE`
- `REQUIRE_CONFIRMATION` (opcional, default: "true")

**Uso**:

```bash
# Eliminar tablas espec√≠ficas (requiere confirmaci√≥n)
python clickhouse_drop_tables.py default Tabla1,Tabla2,Tabla3

# Eliminar tablas por patr√≥n
python clickhouse_drop_tables.py default "PC_%"
```

**Caracter√≠sticas**:
- Confirmaci√≥n de seguridad por defecto
- Soporta filtros por patr√≥n (LIKE)
- Soporta lista espec√≠fica de tablas
- Opci√≥n para eliminar todas las tablas de la base de datos
- Opci√≥n para omitir confirmaci√≥n (√∫til para scripts automatizados)
- Muestra lista de tablas a eliminar antes de confirmar

**Salida**: Tablas eliminadas en ClickHouse

---

## Flujos de Trabajo Comunes

### Flujo 1: SQL Server -> CSV -> Snowflake

```bash
# 1. Exportar tablas de SQL Server a CSV
python sqlserver_to_csv.py MiBaseDeDatos

# 2. (Opcional) Comprimir CSV
python compress_csv_to_gz.py

# 3. Cargar CSV a Snowflake
python csv_to_snowflake.py POM_TEST01 RAW

# 4. (Opcional) Crear tablas individuales desde el stage
python snowflake_csv_to_tables.py POM_TEST01 RAW
```

### Flujo 2: Excel -> CSV -> ClickHouse

```bash
# 1. Convertir Excel a CSV
python excel_to_csv.py

# 2. (Opcional) Comprimir CSV
python compress_csv_to_gz.py

# 3. Cargar CSV a ClickHouse
python csv_to_clickhouse.py default
```

### Flujo 3: Excel -> Snowflake (Directo)

```bash
# Procesar Excel directamente a Snowflake
python ingest_all_excels_to_stage.py
```

### Flujo 4: SQL Server -> Snowflake (Streaming Directo)

```bash
# Exportar directamente desde SQL Server a Snowflake (sin CSV intermedio)
python sqlserver_to_snowflake_streaming.py POM_DBS POM_TEST01 RAW
```

### Flujo 5: SQL Server -> ClickHouse (Streaming Directo con Incremental) 

```bash
# Primera ejecuci√≥n: carga inicial
python sqlserver_to_clickhouse_streaming.py POM_Aplicaciones POM_Aplicaciones_test "PC_Gestiones"

# Ejecuciones posteriores: solo carga registros nuevos/actualizados
python sqlserver_to_clickhouse_streaming.py POM_Aplicaciones POM_Aplicaciones_test "PC_Gestiones"
```

**Ventajas del modo incremental**:
- Solo procesa registros nuevos (m√°s r√°pido)
- Detecta updates autom√°ticamente (lookback window)
- Deduplicaci√≥n autom√°tica con ReplacingMergeTree
- Funciona incluso sin ID ni fecha (modo hash)

### Flujo 6: CSV -> ClickHouse (Crear Tablas Individuales)

```bash
# Crear tablas individuales desde CSV locales
python clickhouse_csv_to_tables.py default
```

### Flujo 7: Procesamiento Completo Automatizado

```bash
# Script batch (ejemplo para Windows)
@echo off
echo Exportando SQL Server...
python sqlserver_to_csv.py MiBaseDeDatos
echo Comprimiendo CSV...
python compress_csv_to_gz.py
echo Cargando a Snowflake...
python csv_to_snowflake.py POM_TEST01 RAW
echo Creando tablas individuales...
python snowflake_csv_to_tables.py POM_TEST01 RAW
echo Proceso completado!
```

### Flujo 8: Limpieza de Tablas

```bash
# Eliminar tablas espec√≠ficas en Snowflake
python snowflake_drop_tables.py POM_TEST01 RAW Tabla1,Tabla2

# Eliminar tablas por patr√≥n en ClickHouse
python clickhouse_drop_tables.py default "TMP_%"
```

---

## Troubleshooting

### Error: "CH_PASSWORD es obligatorio"

**Soluci√≥n**: 
- Crea un archivo `.env` con `CH_PASSWORD=tu_password`
- O define la variable de entorno: `set CH_PASSWORD=tu_password` (Windows) o `export CH_PASSWORD=tu_password` (Linux/Mac)
- Instala `python-dotenv`: `pip install python-dotenv`

### Error: "No se encontr√≥ un driver ODBC compatible"

**Soluci√≥n**: Instala uno de los drivers ODBC para SQL Server:
- [ODBC Driver 17 for SQL Server](https://docs.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server)
- [SQL Server Native Client](https://docs.microsoft.com/en-us/sql/relational-databases/native-client/applications/installing-sql-server-native-client)

### Error: "Error de autenticaci√≥n" en SQL Server

**Soluci√≥n**: 
- **Por defecto se requiere autenticaci√≥n SQL Server**: Define `SQL_USER` y `SQL_PASSWORD` en `.env`
- Si quieres usar autenticaci√≥n Windows, define `SQL_USE_WINDOWS_AUTH=true` en `.env`
- Verifica que tengas permisos en SQL Server
- Si usas autenticaci√≥n SQL, verifica que `SQL_USER` y `SQL_PASSWORD` sean correctos
- Si usas autenticaci√≥n Windows, verifica que tu usuario tenga acceso a SQL Server

### Error: "La base de datos no existe" en Snowflake

**Soluci√≥n**:
- Verifica el nombre exacto de la base de datos (case-sensitive)
- Verifica que tengas permisos para acceder a la base de datos
- El script intentar√° crear la base de datos si no existe (requiere permisos)

### Error: "No se pudieron leer los headers" en ClickHouse

**Soluci√≥n**:
- Verifica que el CSV tenga headers en la primera fila
- Verifica que el delimitador sea correcto (coma, punto y coma, etc.)
- Verifica que el archivo no est√© corrupto

### Error: "PermissionError" al mover archivos

**Soluci√≥n**:
- Verifica que el archivo no est√© abierto en otro programa
- Verifica permisos de escritura en las carpetas destino
- El script intenta autom√°ticamente con reintentos

### Archivos no se procesan

**Soluci√≥n**:
- Verifica que los archivos est√©n en las carpetas correctas (`inbox/`, `csv_staging/`)
- Verifica filtros (`FOLDERS_FILTER`, `CSV_FILTER`, `SHEETS_ALLOWLIST`)
- Verifica que los archivos tengan las extensiones correctas (`.xlsx`, `.csv`, `.csv.gz`)

### Error: "Sorting key contains nullable columns" en ClickHouse

**Soluci√≥n**:
- Este error ocurre cuando una columna nullable se usa en ORDER BY
- El script ahora maneja esto autom√°ticamente usando `ingested_at` como fallback
- Si persiste, verifica que la columna ID no sea nullable en SQL Server

---

## Notas Adicionales

### Sanitizaci√≥n de Nombres

Todos los scripts sanitizan autom√°ticamente los nombres de:
- Archivos CSV
- Nombres de tablas
- Nombres de columnas
- Nombres de carpetas

Caracteres especiales se reemplazan por guiones bajos (`_`).

### Encoding

Todos los archivos CSV se generan con encoding **UTF-8** para soportar caracteres especiales.

### Manejo de NULL/NaN

Los valores NULL/NaN se manejan correctamente:
- En CSV: se convierten a cadenas vac√≠as o se mantienen como NULL seg√∫n el destino
- En Snowflake: se manejan como NULL
- En ClickHouse: se manejan como cadenas vac√≠as o NULL seg√∫n el tipo de columna

### Logging

Los scripts proporcionan informaci√≥n detallada:
- Operaciones exitosas
- Advertencias
- Errores
- Estad√≠sticas (filas, columnas, tama√±o de archivos)
- Tiempo de procesamiento y velocidad (en scripts de streaming)

---

## Changelog

### Versi√≥n 2.1.0 - 19 de enero de 2026

**Cambios en autenticaci√≥n SQL Server (todos los scripts)**:
- [OK] **Autenticaci√≥n SQL Server por defecto**: Ahora requiere `SQL_USER` y `SQL_PASSWORD` por defecto
- [OK] **Variable `SQL_USE_WINDOWS_AUTH`**: Define `SQL_USE_WINDOWS_AUTH=true` para usar autenticaci√≥n Windows
- [OK] **Aplicado a todos los ETLs**: `sqlserver_to_csv.py`, `sqlserver_to_json.py`, `sqlserver_to_clickhouse_streaming.py`, `sqlserver_to_snowflake_streaming.py`
- [OK] **Script de test actualizado**: `sqlserver_test_connection.py` ahora tambi√©n usa SQL Auth por defecto
- [OK] **Mensajes de error mejorados**: Indican claramente qu√© tipo de autenticaci√≥n se est√° usando

### Versi√≥n 2.0.0 - 19 de enero de 2026

**Mejoras en `sqlserver_to_clickhouse_streaming.py`**:
- [OK] Carga incremental inteligente (ID, timestamp, hash)
- [OK] Deduplicaci√≥n autom√°tica con ReplacingMergeTree
- [OK] Lookback window para detectar updates/deletes
- [OK] Verificaci√≥n de hashes por chunk (escalable)
- [OK] Normalizaci√≥n expl√≠cita de valores para hashing
- [OK] ORDER BY correcto seg√∫n modo incremental
- [OK] Eliminado pandas innecesario (mejor rendimiento)
- [OK] Medici√≥n de tiempo y velocidad por chunk
- [OK] Soporte para archivo `.env` con `python-dotenv`
- [OK] Validaci√≥n de credenciales obligatorias
- [OK] Manejo correcto de DateTime64 con timezone
- [OK] Manejo de columnas nullable en ORDER BY

### Versi√≥n 1.0.0 - 19 de enero de 2026

**Scripts incluidos**:
- `sqlserver_to_csv.py` v1.0.0
- `excel_to_csv.py` v1.0.0
- `compress_csv_to_gz.py` v1.0.0
- `csv_to_snowflake.py` v1.0.0
- `csv_to_clickhouse.py` v1.0.0
- `ingest_all_excels_to_stage.py` v1.0.0
- `snowflake_csv_to_tables.py` v1.0.0
- `sqlserver_to_snowflake_streaming.py` v1.0.0
- `sqlserver_to_clickhouse_streaming.py` v1.0.0
- `clickhouse_csv_to_tables.py` v1.0.0
- `snowflake_drop_tables.py` v1.0.0
- `clickhouse_drop_tables.py` v1.0.0

**Caracter√≠sticas iniciales**:
- Exportaci√≥n de tablas SQL Server a CSV
- Conversi√≥n de Excel a CSV
- Compresi√≥n de CSV a formato GZ
- Carga autom√°tica a Snowflake y ClickHouse
- Streaming directo SQL Server -> Snowflake/ClickHouse
- Creaci√≥n de tablas individuales desde CSV
- Eliminaci√≥n segura de tablas con confirmaci√≥n
- Filtrado flexible por tablas, carpetas y archivos
- Manejo robusto de errores con reintentos
- Logging detallado de operaciones
- Sanitizaci√≥n autom√°tica de nombres
- Exclusi√≥n de tablas por prefijos en SQL Server

---

## Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

---

## Licencia

Este proyecto est√° bajo la Licencia MIT - ver el archivo LICENSE para m√°s detalles.

---

## Contacto y Soporte

**Autor**: Herbert Poveda  
**Empresa**: POM Cobranzas  
**Departamento**: Business Intelligence (BI)  
**Fecha**: 19 de enero de 2026

Para preguntas o problemas:
- Abre un issue en el repositorio
- Revisa la secci√≥n [Troubleshooting](#troubleshooting)
- Verifica las variables de entorno y configuraci√≥n en `.env`

---

**Desarrollado por Business Intelligence (BI) - POM Cobranzas**
